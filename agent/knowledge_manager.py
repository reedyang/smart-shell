import os
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Optional, Any
from datetime import datetime
import logging

# çŸ¥è¯†åº“ç›¸å…³å¯¼å…¥
try:
    import chromadb
    from chromadb.config import Settings
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.document_loaders import (
        TextLoader, 
        PDFMinerLoader, 
        Docx2txtLoader,
        UnstructuredMarkdownLoader,
        UnstructuredCSVLoader,
        UnstructuredExcelLoader
    )
    try:
        from langchain_ollama import OllamaEmbeddings
    except ImportError:
        from langchain_community.embeddings import OllamaEmbeddings
    KNOWLEDGE_AVAILABLE = True
except ImportError as e:
    KNOWLEDGE_AVAILABLE = False
    print(f"âš ï¸ çŸ¥è¯†åº“åŠŸèƒ½ä¸å¯ç”¨ï¼Œç¼ºå°‘ä¾èµ–: {e}")

class KnowledgeManager:
    """çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self, config_dir: str, embedding_model: str = "nomic-embed-text"):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“ç®¡ç†å™¨
        Args:
            config_dir: é…ç½®æ–‡ä»¶ç›®å½•
            embedding_model: å‘é‡åŒ–æ¨¡å‹åç§°
        """
        if not KNOWLEDGE_AVAILABLE:
            raise ImportError("çŸ¥è¯†åº“åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·å®‰è£…ç›¸å…³ä¾èµ–")
            
        self.config_dir = Path(config_dir)
        self.knowledge_dir = self.config_dir / "knowledge"
        self.db_dir = self.config_dir / "knowledge_db"
        self.embedding_model = embedding_model
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.knowledge_dir.mkdir(exist_ok=True)
        self.db_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–Chromaæ•°æ®åº“
        self._init_chroma_db()
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        self.supported_extensions = {
            '.txt': TextLoader,
            '.md': UnstructuredMarkdownLoader,
            '.pdf': PDFMinerLoader,
            '.docx': Docx2txtLoader,
            '.csv': UnstructuredCSVLoader,
            '.xlsx': UnstructuredExcelLoader,
            '.xls': UnstructuredExcelLoader,
            '.json': TextLoader,
            '.py': TextLoader,
            '.js': TextLoader,
            '.html': TextLoader,
            '.htm': TextLoader,
            '.xml': TextLoader,
            '.yaml': TextLoader,
            '.yml': TextLoader,
            '.ini': TextLoader,
            '.cfg': TextLoader,
            '.conf': TextLoader,
            '.log': TextLoader
        }
        
        # æ–‡æ¡£çŠ¶æ€è®°å½•æ–‡ä»¶
        self.status_file = self.config_dir / "knowledge_status.json"
        self.document_status = self._load_document_status()
        
    def _init_chroma_db(self):
        """åˆå§‹åŒ–Chromaæ•°æ®åº“"""
        try:
            # åˆå§‹åŒ–OllamaåµŒå…¥æ¨¡å‹
            self.embeddings = OllamaEmbeddings(model=self.embedding_model)
            
            # åˆå§‹åŒ–Chromaå®¢æˆ·ç«¯
            self.client = chromadb.PersistentClient(
                path=str(self.db_dir),
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            self.collection = self.client.get_or_create_collection(
                name="smart_shell_knowledge",
                metadata={"hnsw:space": "cosine"}
            )
            
            print(f"âœ… çŸ¥è¯†åº“åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {self.embedding_model}")
            
        except Exception as e:
            print(f"âŒ çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _load_document_status(self) -> Dict[str, Any]:
        """åŠ è½½æ–‡æ¡£çŠ¶æ€è®°å½•"""
        if self.status_file.exists():
            try:
                with open(self.status_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ–‡æ¡£çŠ¶æ€å¤±è´¥: {e}")
        return {}
    
    def _save_document_status(self):
        """ä¿å­˜æ–‡æ¡£çŠ¶æ€è®°å½•"""
        try:
            with open(self.status_file, 'w', encoding='utf-8') as f:
                json.dump(self.document_status, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ–‡æ¡£çŠ¶æ€å¤±è´¥: {e}")
    
    def _get_file_hash(self, file_path: Path) -> str:
        """è·å–æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return ""
    
    def _get_file_info(self, file_path: Path) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        try:
            stat = file_path.stat()
            return {
                "path": str(file_path),
                "name": file_path.name,
                "size": stat.st_size,
                "modified_time": stat.st_mtime,
                "hash": self._get_file_hash(file_path)
            }
        except Exception:
            return {}
    
    def _load_document(self, file_path: Path) -> Optional[str]:
        """åŠ è½½æ–‡æ¡£å†…å®¹"""
        try:
            extension = file_path.suffix.lower()
            if extension not in self.supported_extensions:
                return None
            
            loader_class = self.supported_extensions[extension]
            
            # ç‰¹æ®Šå¤„ç†æŸäº›æ–‡ä»¶ç±»å‹
            if extension in ['.txt', '.py', '.js', '.html', '.htm', '.xml', '.yaml', '.yml', '.ini', '.cfg', '.conf', '.log', '.json']:
                # æ–‡æœ¬æ–‡ä»¶ç›´æ¥è¯»å–
                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                    return f.read()
            else:
                # ä½¿ç”¨LangChainåŠ è½½å™¨
                loader = loader_class(str(file_path))
                documents = loader.load()
                return "\n\n".join([doc.page_content for doc in documents])
                
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
            return None
    
    def _add_document_to_db(self, file_info: Dict[str, Any], content: str):
        """å°†æ–‡æ¡£æ·»åŠ åˆ°æ•°æ®åº“"""
        try:
            # åˆ†å‰²æ–‡æœ¬
            chunks = self.text_splitter.split_text(content)
            
            # ä¸ºæ¯ä¸ªchunkç”ŸæˆID
            chunk_ids = [f"{file_info['name']}_{i}" for i in range(len(chunks))]
            
            # æ·»åŠ å…ƒæ•°æ®
            metadatas = [{
                "source": file_info['name'],
                "file_path": file_info['path'],
                "chunk_index": i,
                "file_size": file_info['size'],
                "modified_time": file_info['modified_time']
            } for i in range(len(chunks))]
            
            # åˆ†æ‰¹æ·»åŠ åˆ°Chromaæ•°æ®åº“ï¼Œé¿å…è¶…æ—¶
            batch_size = 10
            for i in range(0, len(chunks), batch_size):
                batch_chunks = chunks[i:i+batch_size]
                batch_ids = chunk_ids[i:i+batch_size]
                batch_metadatas = metadatas[i:i+batch_size]
                
                self.collection.add(
                    documents=batch_chunks,
                    metadatas=batch_metadatas,
                    ids=batch_ids
                )
            
            print(f"  âœ… æ·»åŠ æ–‡æ¡£: {file_info['name']} ({len(chunks)} ä¸ªç‰‡æ®µ)")
            
        except Exception as e:
            print(f"  âŒ æ·»åŠ æ–‡æ¡£åˆ°æ•°æ®åº“å¤±è´¥ {file_info['name']}: {e}")
            # å¦‚æœæ˜¯ç½‘ç»œè¶…æ—¶ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°åµŒå…¥
            if "timeout" in str(e).lower() or "handshake" in str(e).lower():
                print(f"  ğŸ’¡ ç½‘ç»œè¶…æ—¶ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹...")
                self._add_document_with_local_embedding(file_info, content)
    
    def _add_document_with_local_embedding(self, file_info: Dict[str, Any], content: str):
        """ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹æ·»åŠ æ–‡æ¡£ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        try:
            from sentence_transformers import SentenceTransformer
            
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
            model = SentenceTransformer('all-MiniLM-L6-v2')
            
            # åˆ†å‰²æ–‡æœ¬
            chunks = self.text_splitter.split_text(content)
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embeddings = model.encode(chunks)
            
            # ä¸ºæ¯ä¸ªchunkç”ŸæˆID
            chunk_ids = [f"{file_info['name']}_{i}" for i in range(len(chunks))]
            
            # æ·»åŠ å…ƒæ•°æ®
            metadatas = [{
                "source": file_info['name'],
                "file_path": file_info['path'],
                "chunk_index": i,
                "file_size": file_info['size'],
                "modified_time": file_info['modified_time']
            } for i in range(len(chunks))]
            
            # æ·»åŠ åˆ°Chromaæ•°æ®åº“
            self.collection.add(
                documents=chunks,
                embeddings=embeddings.tolist(),
                metadatas=metadatas,
                ids=chunk_ids
            )
            
            print(f"  âœ… ä½¿ç”¨æœ¬åœ°åµŒå…¥æ·»åŠ æ–‡æ¡£: {file_info['name']} ({len(chunks)} ä¸ªç‰‡æ®µ)")
            
        except Exception as e:
            print(f"  âŒ æœ¬åœ°åµŒå…¥æ·»åŠ æ–‡æ¡£å¤±è´¥ {file_info['name']}: {e}")
    
    def _remove_document_from_db(self, file_name: str):
        """ä»æ•°æ®åº“ä¸­åˆ é™¤æ–‡æ¡£"""
        try:
            # æŸ¥æ‰¾å¹¶åˆ é™¤æ‰€æœ‰ç›¸å…³çš„chunk
            results = self.collection.get(
                where={"source": file_name}
            )
            
            if results['ids']:
                self.collection.delete(ids=results['ids'])
                print(f"  âœ… åˆ é™¤æ–‡æ¡£: {file_name}")
            
        except Exception as e:
            print(f"  âŒ ä»æ•°æ®åº“åˆ é™¤æ–‡æ¡£å¤±è´¥ {file_name}: {e}")
    
    def sync_knowledge_base(self):
        """åŒæ­¥çŸ¥è¯†åº“"""
        print("ğŸ”„ å¼€å§‹åŒæ­¥çŸ¥è¯†åº“...")
        
        # è·å–å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
        current_files = {}
        for file_path in self.knowledge_dir.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in self.supported_extensions:
                file_info = self._get_file_info(file_path)
                if file_info:
                    current_files[file_path.name] = file_info
        
        # æ£€æŸ¥éœ€è¦åˆ é™¤çš„æ–‡ä»¶
        for file_name in list(self.document_status.keys()):
            if file_name not in current_files:
                print(f"ğŸ—‘ï¸ å‘ç°å·²åˆ é™¤çš„æ–‡æ¡£: {file_name}")
                self._remove_document_from_db(file_name)
                del self.document_status[file_name]
        
        # æ£€æŸ¥éœ€è¦æ·»åŠ æˆ–æ›´æ–°çš„æ–‡ä»¶
        for file_name, file_info in current_files.items():
            if file_name not in self.document_status:
                # æ–°æ–‡ä»¶
                print(f"ğŸ“„ å‘ç°æ–°æ–‡æ¡£: {file_name}")
                content = self._load_document(Path(file_info['path']))
                if content:
                    self._add_document_to_db(file_info, content)
                    self.document_status[file_name] = file_info
            else:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                old_info = self.document_status[file_name]
                if (file_info['modified_time'] != old_info['modified_time'] or 
                    file_info['hash'] != old_info['hash']):
                    print(f"ğŸ”„ å‘ç°æ›´æ–°çš„æ–‡æ¡£: {file_name}")
                    # å…ˆåˆ é™¤æ—§ç‰ˆæœ¬
                    self._remove_document_from_db(file_name)
                    # æ·»åŠ æ–°ç‰ˆæœ¬
                    content = self._load_document(Path(file_info['path']))
                    if content:
                        self._add_document_to_db(file_info, content)
                        self.document_status[file_name] = file_info
        
        # ä¿å­˜çŠ¶æ€
        self._save_document_status()
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        total_docs = len(self.document_status)
        total_chunks = self.collection.count()
        print(f"ğŸ“Š çŸ¥è¯†åº“åŒæ­¥å®Œæˆ: {total_docs} ä¸ªæ–‡æ¡£, {total_chunks} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
    
    def search_knowledge(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        æœç´¢çŸ¥è¯†åº“
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            if not query.strip():
                return []
            
            # æ‰§è¡Œæœç´¢
            results = self.collection.query(
                query_texts=[query],
                n_results=top_k
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    metadata = results['metadatas'][0][i] if results['metadatas'] and results['metadatas'][0] else {}
                    formatted_results.append({
                        'content': doc,
                        'source': metadata.get('source', 'unknown'),
                        'file_path': metadata.get('file_path', ''),
                        'chunk_index': metadata.get('chunk_index', 0),
                        'similarity': results['distances'][0][i] if results['distances'] and results['distances'][0] else 0
                    })
            
            return formatted_results
            
        except Exception as e:
            print(f"âš ï¸ çŸ¥è¯†åº“æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_knowledge_context(self, query: str, max_length: int = 2000) -> str:
        """
        è·å–çŸ¥è¯†åº“ä¸Šä¸‹æ–‡
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            max_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        results = self.search_knowledge(query, top_k=5)
        
        if not results:
            return ""
        
        context_parts = []
        current_length = 0
        
        for result in results:
            content = result['content']
            source = result['source']
            
            # ä¼°ç®—é•¿åº¦ï¼ˆä¸­æ–‡å­—ç¬¦æŒ‰2ä¸ªå­—ç¬¦è®¡ç®—ï¼‰
            content_length = len(content.encode('utf-8'))
            
            if current_length + content_length > max_length:
                break
            
            context_parts.append(f"ã€æ¥æº: {source}ã€‘\n{content}")
            current_length += content_length
        
        if context_parts:
            return "\n\n".join(context_parts)
        else:
            return ""
    
    def get_knowledge_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            total_chunks = self.collection.count()
            total_docs = len(self.document_status)
            
            # æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡
            file_types = {}
            for file_name in self.document_status.keys():
                ext = Path(file_name).suffix.lower()
                file_types[ext] = file_types.get(ext, 0) + 1
            
            return {
                "total_documents": total_docs,
                "total_chunks": total_chunks,
                "file_types": file_types,
                "supported_extensions": list(self.supported_extensions.keys())
            }
        except Exception as e:
            print(f"âš ï¸ è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
